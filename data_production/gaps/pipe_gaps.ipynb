{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS Gaps Pipeline\n",
    "\n",
    "**Last updated:** 2020-11-14\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook executes the following series of three queries to produce an events table of 6+ hour gaps in AIS signal:\n",
    "\n",
    "+ `ais_off_events.sql.j2`: This query identifies the start of gaps in AIS signal longer than a certain number of hours. Results are saved to the date-partitioned table `gfw_research_precursors.ais_off_events_vYYYYMMDD`\n",
    "\n",
    "\n",
    "+ `ais_on_events.sql.j2`: This query identifies the end of the gaps in AIS signal identified by the `ais_off_events.sql.j2` query. Results are saved to the table `gfw_research_precursors.ais_on_events_vYYYYMMDD`\n",
    "\n",
    "\n",
    "+ `ais_gap_events.sql`: This query stitches together the AIS off and on events identified by `ais_off_events.sql.j2` and `ais_on_events.sql.j2`, respectively, and saves/updates results in the table `gfw_research.ais_gap_events_vYYYYMMDD`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Definitions\n",
    "\n",
    "Define the names of the BigQuery datasets and tables to create and/or query\n",
    "\n",
    "TODO: Properly version the tables (currently hardcoded below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "## Development\n",
    "gfw_research = 'scratch_jenn'\n",
    "gfw_research_precursors = 'scratch_jenn'\n",
    "destination_dataset = 'scratch_jenn'\n",
    "\n",
    "# Deployment\n",
    "# gfw_research = 'gfw_research'\n",
    "# gfw_research_precursors = 'gfw_research_precursors'\n",
    "# destination_dataset = 'gfw_research'\n",
    "\n",
    "# Tables\n",
    "pipeline_table = 'pipe_v20190502'\n",
    "segs_table = 'pipe_v20190502_segs'\n",
    "off_events_table = 'ais_off_events_v20201124'\n",
    "on_events_table = 'ais_on_events_v20201124'\n",
    "gap_events_table = 'ais_gap_events_v20201124'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The first step in the AIS gaps pipeline is to determine the date range over which to identify AIS gaps. This includes dates that have already been processed by the pipeline but which have not been added to `gfw_research_precursors.ais_off_events_vYYYYMMDD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import os\n",
    "import pandas_gbq\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "\n",
    "# BigQuery client\n",
    "# client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Write function to find already processed dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def already_processed(dataset = \"gfw_research\", table = \"pipe_v20190502\", partition = 'date'):\n",
    "    \n",
    "    '''\n",
    "    this checks the tables present in a dataset and returns an empty list if\n",
    "    '''  \n",
    "    q = '''SELECT * \n",
    "    FROM {dataset}.__TABLES__\n",
    "    WHERE table_id = '{table}'\n",
    "    '''.format(table=table, dataset=dataset)\n",
    "    df = pandas_gbq.read_gbq(q, project_id=\"world-fishing-827\")\n",
    "    if len(df)==0:\n",
    "        return []\n",
    "    \n",
    "    '''\n",
    "    this returns a list of strings, in format YYYY-MM-DD, that already exist as \n",
    "    date in a given dataset and table\n",
    "    '''    \n",
    "    q = '''\n",
    "    SELECT DISTINCT\n",
    "    {partition} as date\n",
    "    FROM {dataset}.{table}\n",
    "    GROUP BY date \n",
    "    ORDER BY date\n",
    "    '''.format(table=table, dataset=dataset,partition=partition)\n",
    "    df = pandas_gbq.read_gbq(q, project_id=\"world-fishing-827\")\n",
    "    dt = list(df.date)    \n",
    "    ap = map(lambda x: x.strftime(\"%Y-%m-%d\"), dt)\n",
    "    return list(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_commands_in_parallel(commands):\n",
    "    '''This takes a list of commands and runs them in parallel\n",
    "    Note that this assumes you can run 16 commands in parallel,\n",
    "    your mileage may vary if your computer is old and slow.\n",
    "    Requires having gnu parallel installed on your machine.\n",
    "    '''\n",
    "    with open('commands.txt', 'w') as f:\n",
    "        print(commands)\n",
    "        f.write(\"\\n\".join(commands))    \n",
    "    \n",
    "    os.system(\"parallel -j 16 < commands.txt\")\n",
    "    os.system(\"rm -f commands.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the already processed dates in `gfw_research.pipe_v20190502` and `gfw_research_precursors.ais_off_events`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of already processed pipeline dates\n",
    "ap_pipe = already_processed(dataset = \"gfw_research\", table = pipeline_table, partition='date')\n",
    "# print(len(ap_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of off and on event dates processed by gaps pipeline\n",
    "ap_off = already_processed(dataset = gfw_research, table = off_events_table, partition='_partitiontime')\n",
    "print(len(ap_off))\n",
    "\n",
    "ap_on = already_processed(dataset = gfw_research, table = on_events_table, partition='_partitiontime')\n",
    "print(len(ap_on))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the gap off events table doesn't exist, create the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(ap_off)==0):\n",
    "    # if the length of ap_off is zero, that means the off events table doesn't have any data in it, so we must create it.\n",
    "    cmd = \"bq mk --time_partitioning_type=DAY {}.{}\".format(gfw_research_precursors, off_events_table)\n",
    "    os.system(cmd)\n",
    "\n",
    "if(len(ap_on)==0):\n",
    "    # if the length of ap_on is zero, that means the on events table doesn't have any data in it, so we must create it.\n",
    "    cmd = \"bq mk --time_partitioning_type=DAY {}.{}\".format(gfw_research_precursors, on_events_table)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all dates to process by finding dates in `ap_pipe` not in `ap_gaps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values to process\n",
    "# NOTE -- we don't' want to process the last day\n",
    "# bedause we dan't calculate off events without a following day\n",
    "tp = [t for t in ap_pipe[:-1] if t not in ap_gaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Used for development to run discrete time periods\n",
    "\n",
    "# from datetime import timedelta, date\n",
    "\n",
    "# def daterange(date1, date2):\n",
    "#     for n in range(int ((date2 - date1).days)+1):\n",
    "#         yield date1 + timedelta(n)\n",
    "\n",
    "# start_dt = date(2012, 1, 1)\n",
    "# end_dt = date(2012, 1, 31)\n",
    "# tp = []\n",
    "# for dt in daterange(start_dt, end_dt):\n",
    "#     tp.append(dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function on first date\n",
    "if len(tp) <= 31:\n",
    "    for t in tp:\n",
    "        print(str(t))\n",
    "else:\n",
    "    print(\"Start:\", tp[0])\n",
    "    print(\"End:\", tp[-1])\n",
    "    print(\"Number of days:\", len(tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the off events query to validate and/or edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ais_off_on_events.sql.j2 file\n",
    "with open('ais_off_on_events.sql.j2') as f:\n",
    "    sql_template = Template(f.read())\n",
    "\n",
    "# Format the query according to the desired event type, date, and gap length\n",
    "output_template = sql_template.render( pipeline_table=\"{}.{}\".format(\"gfw_research\", pipeline_table), segs_table=\"{}.{}\".format(\"gfw_research\", segs_table), event='off', date=\"2012-01-03\", min_gap_length=6)\n",
    "\n",
    "print(output_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the off events tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off/on events function\n",
    "def make_ais_events_table(pipeline_table, segs_table, event_type, date, min_gap_hours, precursors_dataset, destination_table):\n",
    "    \n",
    "    # Format date string without dashes for partition\n",
    "    dest_table_partition = destination_table + \"\\$\"+ date.replace(\"-\",\"\")\n",
    "    \n",
    "    # Format jinja2 command\n",
    "    cmd = \"\"\"jinja2 ais_off_on_events.sql.j2 \\\n",
    "    -D pipeline_table=\"{pipeline_table}\" \\\n",
    "    -D segs_table=\"{segs_table}\" \\\n",
    "    -D event=\"{event_type}\" \\\n",
    "    -D date=\"{date}\" \\\n",
    "    -D min_gap_length={min_gap_hours} \\\n",
    "    | \\\n",
    "    bq query --replace \\\n",
    "    --destination_table={precursors_dataset}.{destination_table}\\\n",
    "    --allow_large_results --use_legacy_sql=false --max_rows=0\n",
    "    \"\"\".format(pipeline_table=pipeline_table,\n",
    "               segs_table=segs_table,\n",
    "               event_type=event_type,\n",
    "               date=date, \n",
    "               min_gap_hours=min_gap_hours,\n",
    "               precursors_dataset=precursors_dataset,\n",
    "               destination_table=dest_table_partition)\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS Off Events\n",
    "\n",
    "Generate all off event commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "cmds = []\n",
    "for t in tp:\n",
    "    cmd = make_ais_events_table(pipeline_table=\"{}.{}\".format(\"gfw_research\", pipeline_table),\n",
    "                                segs_table=\"{}.{}\".format(\"gfw_research\", segs_table),\n",
    "                                event_type='off',\n",
    "                                date = t,\n",
    "                                min_gap_hours=6, \n",
    "                                precursors_dataset=gfw_research_precursors,\n",
    "                                destination_table=off_events_table)\n",
    "    cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all commands\n",
    "# print(cmds[0])\n",
    "execute_commands_in_parallel(commands=cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS On Events\n",
    "\n",
    "Generate all on event commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "cmds = []\n",
    "for t in tp:\n",
    "    cmd = make_ais_events_table(pipeline_table=\"{}.{}\".format(\"gfw_research\", pipeline_table),\n",
    "                                segs_table=\"{}.{}\".format(\"gfw_research\", segs_table),\n",
    "                                event_type='on',\n",
    "                                date = t,\n",
    "                                min_gap_hours=6, \n",
    "                                precursors_dataset=gfw_research_precursors,\n",
    "                                destination_table=on_events_table)\n",
    "    cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all commands\n",
    "# print(cmds[0])\n",
    "execute_commands_in_parallel(commands=cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS Gap Events\n",
    "\n",
    "Combine the tables of AIS off events and AIS on events into a single table of completed AIS gap events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ais_off_on_events.sql.j2 file\n",
    "with open('ais_gap_events.sql.j2') as f:\n",
    "    sql_template = Template(f.read())\n",
    "\n",
    "# Format the query according to the desired event type, date, and gap length\n",
    "output_template = sql_template.render(off_events_table=\"{}.{}\".format(gfw_research_precursors,off_events_table),\n",
    "                                      on_events_table=\"{}.{}\".format(gfw_research_precursors,on_events_table),\n",
    "                                      date=\"2017-01-01\")\n",
    "\n",
    "print(output_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if gap events table exists in destination dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gaps table exists in destination dataset\n",
    "gaps_exist = already_processed(dataset = gfw_research, table = gap_events_table, partition = 'gap_start')\n",
    "print(len(gaps_exist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make empty table for gap events if the table does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gaps table if it does not exist\n",
    "if(len(gaps_exist)==0):\n",
    "    # if the length is zero, that means the table doesn't have any data in it.\n",
    "    # create the table so that we can add to it in the next step. \n",
    "    cmd = \"bq mk --schema=ais_gap_events.json --time_partitioning_field=gap_start --time_partitioning_type=DAY {}.{}\".format(gfw_research, gap_events_table)\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to overwrite data in the gap events table. Gap events are partitioned by the gap_start field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off/on events function\n",
    "def make_ais_gap_events_table(off_events_table,\n",
    "                              on_events_table,\n",
    "                              date, \n",
    "                              precursors_dataset,\n",
    "                              destination_dataset, \n",
    "                              destination_table):\n",
    "        \n",
    "    # Format jinja2 command\n",
    "    cmd = \"\"\"jinja2 ais_gap_events.sql.j2 \\\n",
    "    -D off_events_table=\"{precursors_dataset}.{off_events_table}\" \\\n",
    "    -D on_events_table=\"{precursors_dataset}.{on_events_table}\" \\\n",
    "    -D date=\"{date}\" \\\n",
    "    | \\\n",
    "    bq query --replace \\\n",
    "    --destination_table={destination_dataset}.{destination_table}\\\n",
    "    --allow_large_results --use_legacy_sql=false --max_rows=0\n",
    "    \"\"\".format(off_events_table=off_events_table,\n",
    "               on_events_table=on_events_table,\n",
    "               date=date,\n",
    "               precursors_dataset=precursors_dataset,\n",
    "               destination_dataset=destination_dataset,\n",
    "               destination_table=destination_table)\n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate query to make replacement gaps table based on the latest date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Store commands\n",
    "cmds = []\n",
    "latest_date = tp[-1]\n",
    "cmd = make_ais_gap_events_table(off_events_table = off_events_table,\n",
    "                                on_events_table = on_events_table,\n",
    "                                date = latest_date,\n",
    "                                precursors_dataset = gfw_research_precursors,\n",
    "                                destination_dataset = gfw_research,\n",
    "                                destination_table = gap_events_table)\n",
    "cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all commands to update gap events table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(cmds))\n",
    "execute_commands_in_parallel(commands=cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
