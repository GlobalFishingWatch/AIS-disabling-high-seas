{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Smoothed Reception\n",
    "\n",
    "This notebook creates smoothed reception maps at desired time intervals and uploads the tables to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib import colors,colorbar, cm\n",
    "import cmocean\n",
    "\n",
    "import scipy\n",
    "import scipy.interpolate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "# Establish BigQuery connection\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grids(df, names):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "        The data frame should contain gridded data that is gridded\n",
    "        to match inverse_delta_degrees, where lat and lon are\n",
    "        specified as `lat_bin` and `lon_bin`. Whether a given\n",
    "        row corresponds to class A or class B is specified by\n",
    "        the `cls` variable.\n",
    "    names: list of str\n",
    "        Name to extract from the data frame and place in a grid\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A_grids, B_grids : dict of arrays\n",
    "        The dictionary keys are the draw from the passed in names.\n",
    "    \"\"\"\n",
    "    A_grids = {k : np.zeros([n_lat, n_lon]) for k in names}\n",
    "    B_grids = {k : np.zeros([n_lat, n_lon]) for k in names}\n",
    "    for row in df.itertuples():\n",
    "        if np.isnan(row.lat_bin) or np.isnan(row.lon_bin):\n",
    "            continue\n",
    "        lat_ndx = int((row.lat_bin - min_lat) * inverse_delta_degrees)\n",
    "        lon_ndx = int((row.lon_bin - min_lon) * inverse_delta_degrees)\n",
    "        grids = A_grids if (row.cls == 'A') else B_grids\n",
    "        for k in names:\n",
    "            grids[k][lat_ndx][lon_ndx] = getattr(row, k)\n",
    "    return A_grids, B_grids\n",
    "\n",
    "def plot_grid(data, normtype=colors.LogNorm, cmap=cm.viridis, min_val=1, max_val=400, figsize=(12, 6)):\n",
    "    lons = np.arange(min_lon, max_lon+1)\n",
    "    lats = np.arange(min_lat, max_lat+1)\n",
    "\n",
    "    norm = normtype(vmin=min_val, vmax=max_val)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection=ccrs.Robinson())\n",
    "    plt.pcolormesh(lons, lats, data, norm=norm, cmap=cmap, transform = ccrs.PlateCarree())\n",
    "    ax.coastlines(linewidth=.5)\n",
    "    ax.add_feature(cfeature.LAND, zorder=10)\n",
    "    \n",
    "    ax = fig.add_axes([0.35, 0.05, 0.3, 0.02])  \n",
    "    cb = colorbar.ColorbarBase(ax, norm=norm, cmap=cmap, orientation='horizontal')\n",
    "    \n",
    "\n",
    "    return ax\n",
    "\n",
    "def interpolate_reception(y, hours, hours_cap=15, elevation_scale=10, smooth=1, epsilon=None, hours_threshold=0,\n",
    "                rough_draft=True):\n",
    "    \"\"\"\n",
    "    The strategy is to use RBF to interpolate the existing points onto a smooth grid. \n",
    "    There does not seem to be any provision for weighting but we exploit a third dimension \n",
    "    to apply psudo weighting to the data. Ideally we would use a Haversine metric, but there\n",
    "    is no built in haversine metric and using a custom metric is very slow. \n",
    "    As a result, opting for the default Euclidean metric.\n",
    "    \"\"\"\n",
    "    # Create the interpolation grid.\n",
    "    lonvec, latvec = np.meshgrid(np.linspace(min_lon, max_lon, n_lon, endpoint=False), \n",
    "                                 np.linspace(min_lat, max_lat, n_lat, endpoint=False))\n",
    "\n",
    "    y = y.copy()\n",
    "    mask = (hours > hours_threshold)\n",
    "    if rough_draft:\n",
    "        # This thins the mask by a factor of 4\n",
    "        # and is much faster. Useful for experimenting\n",
    "        for i in range(0, 1):\n",
    "            mask[:, i::2] = False\n",
    "            mask[i::2, :] = False\n",
    "\n",
    "    # Implement pseudo weighting by placing points with\n",
    "    # fewer hours \"higher\" than other points so they\n",
    "    # end up farther away, and effectively downweighting.\n",
    "    # At inference time all points have elevation zero.\n",
    "    elevation = elevation_scale * (hours_cap - np.minimum(hours, hours_cap))\n",
    "    \n",
    "    # Asssume good reception at poles\n",
    "    good = np.percentile(y[mask], 90)\n",
    "    mask[0, :] = mask[-1, :] = True\n",
    "    y[0, :] = y[-1, :] = good\n",
    "    elevation[0, :] = elevation[-1, :] = 0\n",
    "    \n",
    "    # Since we are using a Euclidean metric, we paste three copied\n",
    "    # of the data together to avoid a problem at the dateline. We only\n",
    "    # add 20 degrees worth of data to either side to keep the problem size in \n",
    "    # check\n",
    "    west_mask = mask & (lonvec > 120)\n",
    "    east_mask = mask & (lonvec < -120)\n",
    "\n",
    "    latf = np.concatenate([latvec[west_mask].ravel(), latvec[mask].ravel(), latvec[east_mask].ravel()])\n",
    "    elevf = np.concatenate([elevation[west_mask].ravel(), elevation[mask].ravel(), \n",
    "                            elevation[east_mask].ravel()])\n",
    "    yf = np.concatenate([y[west_mask].ravel(), y[mask].ravel(), y[east_mask].ravel()])\n",
    "    # One copy of the longitude data is shifted west and one east.\n",
    "    lonf = np.concatenate([(lonvec[west_mask] - 360).ravel(), lonvec[mask].ravel(), \n",
    "                           (lonvec[east_mask] + 360).ravel()])\n",
    "\n",
    "    # There are two primary knobs to twiddle here, `smooth` and `epsilon`. With `smooth`\n",
    "    # set to zero, it does an exact fit and larger values result in a smoother less \n",
    "    # exact fit. `epsilon` is a scale parameter and also affects smoothness, but I haven't\n",
    "    # played with it as much. The `function` parameter can also be manipulated, but I haven't\n",
    "    # had any luck with anything other than default `multiquadric`. In addition, adjusting\n",
    "    # `elevation_scale` will change the amount of downweighting applied to cells with\n",
    "    # few hours\n",
    "    interpolater = scipy.interpolate.Rbf(lonf, latf, elevf, yf, smooth=smooth, epsilon=epsilon)\n",
    "    \n",
    "    return interpolater(lonvec.flatten(), latvec.flatten(), \n",
    "                                    np.zeros_like(lonvec.flatten())).reshape(180, 360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Query\n",
    "\n",
    "The below query calculates the measured reception quality for the given month prior to interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reception_measured(start_date, end_date):\n",
    "    # TODO: remove fishing only from class A\n",
    "    query = f\"\"\"\n",
    "    CREATE TEMP FUNCTION startdate() AS (DATE('{start_date:%Y-%m-%d}'));\n",
    "    CREATE TEMP FUNCTION enddate() AS (DATE('{end_date:%Y-%m-%d}'));\n",
    "    #### Reception Quality\n",
    "    with \n",
    "\n",
    "    good_ssvid as (\n",
    "        select ssvid\n",
    "        from `gfw_research.vi_ssvid_v20200410` \n",
    "        where best.best_vessel_class not in  (\"gear\", \"squid_jigger\", \"pole_and_line\")\n",
    "          and not activity.offsetting \n",
    "          and activity.active_positions > 1000\n",
    "          and best.best_vessel_class is not null\n",
    "    ),\n",
    "    sat_ssvid as (\n",
    "        select ssvid,\n",
    "               sat_positions,\n",
    "               lat,\n",
    "               lon,\n",
    "               hour,\n",
    "               interpolated_speed_knots,\n",
    "               _partitiontime date ,\n",
    "               A_messages,\n",
    "               B_messages \n",
    "        from `gfw_research_precursors.ais_positions_byssvid_hourly_v20191118`\n",
    "        where not interpolated_at_segment_startorend -- don't extrapolate out at the end of segments\n",
    "          and date(_partitiontime) >= startdate() \n",
    "          and date(_partitiontime) < enddate()\n",
    "          and ssvid in (select ssvid from good_ssvid)\n",
    "    ),\n",
    "    by_half_day as (\n",
    "        select ssvid,\n",
    "               avg(interpolated_speed_knots) avg_interpolated_speed_knots,\n",
    "               min(interpolated_speed_knots) min_interpolated_speed_knots,\n",
    "               max(interpolated_speed_knots) max_interpolated_speed_knots,\n",
    "               sum(sat_positions)/count(*) sat_pos_per_hour,\n",
    "               floor(hour/12) day_half,\n",
    "               sum(A_messages) A_messages,\n",
    "               sum(B_messages) B_messages,\n",
    "               date\n",
    "        from sat_ssvid\n",
    "        group by ssvid, date, day_half\n",
    "    ),\n",
    "    reception_quality as (\n",
    "        select floor(a.lat) lat_bin,\n",
    "               floor(a.lon) lon_bin,\n",
    "               if(by_half_day.A_messages > 0, \"A\", \"B\") class,\n",
    "               count(*) hours,\n",
    "               avg(sat_pos_per_hour) * 24 sat_pos_per_day\n",
    "        from sat_ssvid a\n",
    "        join by_half_day \n",
    "        on    a.ssvid=by_half_day.ssvid \n",
    "          and floor(a.hour/12) = by_half_day.day_half\n",
    "          and a.date = by_half_day.date\n",
    "          and (-- if Class A, moving at the speed to ping once every 10 seconds\n",
    "               (by_half_day.A_messages > 0 and  min_interpolated_speed_knots > 0.5 \n",
    "                and max_interpolated_speed_knots < 14)\n",
    "               or (by_half_day.B_messages > 0 and min_interpolated_speed_knots > 2 ))\n",
    "          -- make sure it is just class A or class B... this might make it fail\n",
    "          -- for the vessels that are both A and B...\n",
    "          and not (by_half_day.A_messages > 0 and by_half_day.B_messages > 0) \n",
    "          and max_interpolated_speed_knots < 30 -- eliminate some weird noise\n",
    "        group by lat_bin, lon_bin, class\n",
    "    )\n",
    "\n",
    "    select class as cls, * except (class) from reception_quality\n",
    "    \"\"\"\n",
    "    ping_density = pd.read_gbq(query, project_id='world-fishing-827', dialect='standard')\n",
    "    \n",
    "    return(ping_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Setup global parameters and time range for generation reception maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time interval for reception maps\n",
    "reception_level = 'month'\n",
    "\n",
    "# Min/Max coordinates \n",
    "min_lon, min_lat, max_lon, max_lat  = -180, -90, 180, 90\n",
    "\n",
    "# Number of lat/lon bins\n",
    "inverse_delta_degrees = 1\n",
    "n_lat = (max_lat - min_lat) * inverse_delta_degrees\n",
    "n_lon = (max_lon - min_lon) * inverse_delta_degrees\n",
    "\n",
    "lons = np.arange(min_lon, max_lon+1)\n",
    "lats = np.arange(min_lat, max_lat+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Setup\n",
    "\n",
    "Create the destination BigQuery table for the smoothed reception maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output location\n",
    "destination_dataset = 'proj_ais_gaps_catena'\n",
    "table_name = 'sat_reception_one_degree_v20200806'\n",
    "destination_table = '{d}.{t}'.format(d=destination_dataset,t=table_name)\n",
    "\n",
    "# BigQuery references\n",
    "dataset_ref = client.dataset(destination_dataset)\n",
    "table_ref = dataset_ref.table(table_name)\n",
    "create_table = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create date partitioned table prior to running interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(create_table):\n",
    "    table = bigquery.Table(table_ref)\n",
    "    table = client.create_table(table)\n",
    "    print(\"Created table {}\".format(table.table_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation and Upload to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range over which to generate reception maps\n",
    "start_date = datetime(2018, 9, 1)\n",
    "end_date = datetime(2019, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (start_date < end_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate smooth reception map for month\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dates for reception map \n",
    "    reception_start = start_date\n",
    "    # End date (not inclusive)\n",
    "    reception_end = start_date + relativedelta(months=1)\n",
    "\n",
    "    ### Measured reception ###\n",
    "    # Query to calculate measured reception\n",
    "    print(\"Querying reception for {}\".format(reception_start))\n",
    "    month_reception = reception_measured(reception_start, reception_end)\n",
    "    \n",
    "    # Generate Class A and B grids from ping_density query results\n",
    "    A_grids, B_grids = make_grids(month_reception, ['sat_pos_per_day', 'hours'])\n",
    "\n",
    "    ### Interpolated reception ###\n",
    "    print(\"Interpolating reception for {}\".format(reception_start))\n",
    "    # Interpolate reception for Class A\n",
    "    smoothed_A_reception = interpolate_reception(A_grids['sat_pos_per_day'], A_grids['hours'])\n",
    "    # Interpolate reception for Class A\n",
    "    smoothed_B_reception = interpolate_reception(B_grids['sat_pos_per_day'], B_grids['hours'])\n",
    "    \n",
    "    ### Plots ###\n",
    "    ax = plot_grid(np.maximum(smoothed_A_reception, 1), max_val=500)\n",
    "    ax.set_title(f\"Smoothed pings per vessel day (class A, {reception_start:%Y-%m})\")\n",
    "    plt.show()\n",
    "    \n",
    "    ax = plot_grid(np.maximum(smoothed_B_reception, 1), max_val=100)\n",
    "    ax.set_title(f\"Smoothed pings per vessel day (class B, {reception_start:%Y-%m})\")\n",
    "    plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert data to pandas data frame and upload to BigQuery\n",
    "    \"\"\"\n",
    "    # Empty list to store coordinates and values\n",
    "    data_A = []\n",
    "    data_B = []\n",
    "\n",
    "    # Loop over lat/lon and fill with positions per hour\n",
    "    for lat in range(n_lat):\n",
    "        for lon in range(n_lon):\n",
    "            # Add data to each list\n",
    "            data_A.append([lats[lat], lons[lon], smoothed_A_reception[lat][lon]])\n",
    "            data_B.append([lats[lat], lons[lon], smoothed_B_reception[lat][lon]])\n",
    "\n",
    "    # Convert lists to pandas dataframes\n",
    "    df_A = pd.DataFrame(data_A, columns=['lat_bin','lon_bin','positions_per_day'])\n",
    "    df_A['class'] = 'A'\n",
    "\n",
    "    df_B = pd.DataFrame(data_B, columns=['lat_bin','lon_bin','positions_per_day'])\n",
    "    df_B['class'] = 'B'\n",
    "\n",
    "    # Stack dataframes into single table\n",
    "    df = pd.concat([df_A, df_B], axis = 0).reset_index(drop=True)\n",
    "\n",
    "    # Add column for year and month\n",
    "    df['year'] = reception_start.year\n",
    "    df['month'] = reception_start.month\n",
    "\n",
    "    # Rearrange columns\n",
    "    df = df[['year','month','lat_bin','lon_bin','class','positions_per_day']]\n",
    "\n",
    "    # Set index to remove index colum\n",
    "    df = df.set_index('year')\n",
    "\n",
    "    # Save data to tmp csv file\n",
    "    df.to_csv('tmp.csv')\n",
    "\n",
    "    # Upload to BigQuery\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.skip_leading_rows = 1\n",
    "    job_config.autodetect = True\n",
    "\n",
    "    with open(\"tmp.csv\", \"rb\") as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "    job.result()  # Waits for table load to complete.\n",
    "\n",
    "    print(\"Loaded {} rows for {} into {}.\".format(job.output_rows, reception_start, table_name))\n",
    "    \n",
    "    # Update month for next iteration\n",
    "    start_date = reception_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.7.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
