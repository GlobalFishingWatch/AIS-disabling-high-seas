{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate AIS reception and gap events datasets\n",
    "\n",
    "This notebook is a wrapper file for producing a complete set of results and inputs for Welch et al. (2021). It contains code for the following: \n",
    "\n",
    "1. Generate raw AIS gap events greater than 12 hours\n",
    "2. Generate monthly AIS reception maps\n",
    "3. Detect suspected AIS disabling events\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/h3/unstable/__init__.py:4: UserWarning: Modules under `h3.unstable` are experimental, and may change at any time.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template\n",
    "\n",
    "import pyseas\n",
    "import pyseas.maps\n",
    "import pyseas.maps.rasters\n",
    "import pyseas.styles\n",
    "import pyseas.cm\n",
    "\n",
    "# project specific functions\n",
    "import utils \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# BigQuery client\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input BQ datasets/tables\n",
    "gfw_research = 'gfw_research'\n",
    "gfw_research_precursors = 'gfw_research_precursors'\n",
    "destination_dataset = 'scratch_tyler'\n",
    "\n",
    "pipeline_version = 'v20201001'\n",
    "pipeline_table = 'pipe_{}'.format(pipeline_version)\n",
    "segs_table = 'pipe_{}_segs'.format(pipeline_version)\n",
    "vi_version = 'v20210706'\n",
    "\n",
    "# Output tables version\n",
    "output_version = 'v20210721'\n",
    "create_tables = True\n",
    "\n",
    "# Date range\n",
    "start_date = date(2017,1, 1)\n",
    "end_date = date(2017,1, 4)\n",
    "\n",
    "# Min gap hours\n",
    "min_gap_hours = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of dates to produce for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of dates to run\n",
    "dates_to_run = utils.daterange(start_date, end_date)\n",
    "tp = []\n",
    "for dt in dates_to_run:\n",
    "    tp.append(dt.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS Gaps dataset\n",
    "\n",
    "Generate a dataset of AIS gaps for time range. This involves running the following query sequence (queries in the `gaps` subdirectory):\n",
    "1. AIS off events: `ais_off_on_events.sql.j2` with `event` parameter set to `'off'`\n",
    "2. AIS on events: `ais_off_on_events.sql.j2` with `event` parameter set to `'on'`\n",
    "3. AIS gap events: Stitch off and on events together into gap events using `ais_gap_events.sql.j2`\n",
    "\n",
    "### Create tables\n",
    "\n",
    "First, create empty tables for all three tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination tables\n",
    "off_events_table = 'ais_off_events_{}'.format(output_version)\n",
    "on_events_table = 'ais_on_events_{}'.format(output_version)\n",
    "gap_events_table = 'ais_gap_events_{}'.format(output_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables for off/on events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tables:\n",
    "    # Off events\n",
    "    utils.make_bq_partitioned_table(destination_dataset, off_events_table)\n",
    "    # On events\n",
    "    utils.make_bq_partitioned_table(destination_dataset, on_events_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off events\n",
    "\n",
    "Generate off events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "cmds = []\n",
    "for t in tp:\n",
    "    cmd = utils.make_ais_events_table(pipeline_table=\"{}.{}\".format(\"gfw_research\", pipeline_table),\n",
    "                                segs_table=\"{}.{}\".format(\"gfw_research\", segs_table),\n",
    "                                event_type='off',\n",
    "                                date = t,\n",
    "                                min_gap_hours = min_gap_hours, \n",
    "                                precursors_dataset=destination_dataset,\n",
    "                                destination_table=off_events_table)\n",
    "    cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test query\n",
    "# test_cmd = cmds[0].split('|')[0]\n",
    "# os.system(test_cmd)\n",
    "# os.system(cmds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"off\"     -D date=\"2017-01-01\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_off_events_v20210721\\\\$20170101    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"off\"     -D date=\"2017-01-02\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_off_events_v20210721\\\\$20170102    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"off\"     -D date=\"2017-01-03\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_off_events_v20210721\\\\$20170103    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"off\"     -D date=\"2017-01-04\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_off_events_v20210721\\\\$20170104    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ']\n"
     ]
    }
   ],
   "source": [
    "# Run queries\n",
    "utils.execute_commands_in_parallel(commands=cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On events\n",
    "\n",
    "Generate on events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "on_cmds = []\n",
    "for t in tp:\n",
    "    cmd = utils.make_ais_events_table(pipeline_table=\"{}.{}\".format(\"gfw_research\", pipeline_table),\n",
    "                                segs_table=\"{}.{}\".format(\"gfw_research\", segs_table),\n",
    "                                event_type='on',\n",
    "                                date = t,\n",
    "                                min_gap_hours = min_gap_hours, \n",
    "                                precursors_dataset=destination_dataset,\n",
    "                                destination_table=on_events_table)\n",
    "    on_cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test query\n",
    "# test_cmd = on_cmds[0].split('|')[0]\n",
    "# os.system(test_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"on\"     -D date=\"2017-01-01\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_on_events_v20210721\\\\$20170101    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"on\"     -D date=\"2017-01-02\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_on_events_v20210721\\\\$20170102    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"on\"     -D date=\"2017-01-03\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_on_events_v20210721\\\\$20170103    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ', 'jinja2 gaps/ais_off_on_events.sql.j2     -D pipeline_table=\"gfw_research.pipe_v20201001\"     -D segs_table=\"gfw_research.pipe_v20201001_segs\"     -D event=\"on\"     -D date=\"2017-01-04\"     -D min_gap_length=6     |     bq query --replace     --destination_table=scratch_tyler.ais_on_events_v20210721\\\\$20170104    --allow_large_results --use_legacy_sql=false --max_rows=0\\n    ']\n"
     ]
    }
   ],
   "source": [
    "# Run queries\n",
    "utils.execute_commands_in_parallel(commands=on_cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap events\n",
    "\n",
    "Combine off and on events into gap events.\n",
    "\n",
    "Create gap events table, partitioning on the `gap_start` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "if create_tables:\n",
    "    gap_tbl_cmd = \"bq mk --schema=gaps/ais_gap_events.json \\\n",
    "    --time_partitioning_field=gap_start \\\n",
    "    --time_partitioning_type=DAY {}.{}\".format(destination_dataset, \n",
    "                                               gap_events_table)\n",
    "    os.system(gap_tbl_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = tp[-1]\n",
    "gap_cmd = utils.make_ais_gap_events_table(off_events_table = off_events_table,\n",
    "                                on_events_table = on_events_table,\n",
    "                                date = latest_date,\n",
    "                                precursors_dataset = destination_dataset,\n",
    "                                destination_dataset = destination_dataset,\n",
    "                                destination_table = gap_events_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test query\n",
    "# test_cmd = gap_cmd.split('|')[0]\n",
    "# os.system(test_cmd)\n",
    "# os.system(test_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run command\n",
    "os.system(gap_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update schema\n",
    "gap_schema_cmd = \"bq update --schema=gaps/ais_gap_events.json {}.{}\".format(destination_dataset, gap_events_table)\n",
    "os.system(gap_schema_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS Interpolation\n",
    "\n",
    "The next step is to generate tables of interpolated vessel positions. These tables are used subsequently for the following:\n",
    "- AIS reception\n",
    "- Time lost to gaps\n",
    "\n",
    "> The original reception quality method used a slightly different interpolation [query](https://github.com/GlobalFishingWatch/ais-gaps-and-reception/blob/master/data-production/hourly_interpoloation_v20191120.sql.j2)/table (`gfw_research_precursors.ais_positions_byssvid_hourly_v20191118`) than the [query](https://github.com/GlobalFishingWatch/ais-gaps-and-reception/blob/master/data-production/pipe-interpolation/hourly_interpoloation_v20201027.sql.j2) used to estimate time lost to gaps. These approaches have been combined/streamlined into the `interpolation/hourly_interpolation_byseg.sql.j2` in this repo.\n",
    "\n",
    "### Create tables\n",
    "\n",
    "First create empty date partitioned tables to store interpolated positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination tables\n",
    "ais_positions_hourly = 'ais_positions_byssvid_hourly_{}'.format(output_version)\n",
    "ais_positions_hourly_fishing = 'ais_positions_byssvid_hourly_fishing_{}'.format(output_version)\n",
    "gap_positions_hourly = 'gap_positions_byssvid_hourly_{}'.format(output_version)\n",
    "loitering_positions_hourly = 'loitering_positions_byssvid_hourly_{}'.format(output_version)\n",
    "\n",
    "# By seg_id\n",
    "ais_positions_hourly = 'ais_positions_byseg_hourly_{}'.format(output_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tables:\n",
    "    # all positions hourly\n",
    "    utils.make_bq_partitioned_table(destination_dataset, ais_positions_hourly)\n",
    "    # fishing vessel positions hourly\n",
    "    utils.make_bq_partitioned_table(destination_dataset, ais_positions_hourly_fishing)\n",
    "    # gap positions hourly\n",
    "    utils.make_bq_partitioned_table(destination_dataset, gap_positions_hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate all vessel positions\n",
    "\n",
    "Interpolate positions for all vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "int_cmds = []\n",
    "for t in tp:\n",
    "    cmd = utils.make_hourly_interpolation_table(date = t,\n",
    "                                                destination_dataset = destination_dataset,\n",
    "                                                destination_table = ais_positions_hourly)\n",
    "    int_cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-01\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170101         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-02\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170102         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-03\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170103         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-04\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170104         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-05\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170105         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-06\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170106         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-07\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170107         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-08\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170108         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-09\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170109         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-10\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170110         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-11\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170111         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-12\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170112         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-13\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170113         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-14\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170114         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-15\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170115         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-16\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170116         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-17\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170117         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-18\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170118         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-19\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170119         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-20\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170120         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-21\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170121         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-22\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170122         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-23\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170123         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-24\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170124         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-25\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170125         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-26\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170126         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-27\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170127         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-28\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170128         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-29\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170129         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-30\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170130         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_interpolation_byseg.sql.j2           -D YYYY_MM_DD=\"2017-01-31\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byseg_hourly_v20210429\\\\$20170131         --allow_large_results --use_legacy_sql=false ']\n"
     ]
    }
   ],
   "source": [
    "utils.execute_commands_in_parallel(int_cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate fishing vessel positions\n",
    "Interpolate positions for fishing vessels, including both `nnet_score` and `night_loitering` for determining when `squid_jiggers` are fishing.\n",
    "\n",
    "TODO: Update this to the same logic as for all vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store commands\n",
    "# int_fishing_cmds = []\n",
    "# for t in tp:\n",
    "#     cmd = utils.make_hourly_fishing_interpolation_table(date = t,\n",
    "#                                                 destination_dataset = destination_dataset,\n",
    "#                                                 destination_table = ais_positions_hourly_fishing)\n",
    "#     int_fishing_cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-01\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170101         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-02\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170102         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-03\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170103         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-04\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170104         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-05\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170105         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-06\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170106         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-07\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170107         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-08\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170108         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-09\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170109         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-10\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170110         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-11\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170111         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-12\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170112         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-13\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170113         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-14\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170114         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-15\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170115         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-16\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170116         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-17\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170117         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-18\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170118         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-19\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170119         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-20\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170120         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-21\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170121         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-22\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170122         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-23\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170123         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-24\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170124         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-25\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170125         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-26\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170126         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-27\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170127         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-28\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170128         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-29\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170129         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-30\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170130         --allow_large_results --use_legacy_sql=false ', 'jinja2 interpolation/hourly_fishing_interpolation.sql.j2           -D YYYY_MM_DD=\"2017-01-31\"        |         bq query --replace         --destination_table=scratch_tyler.ais_positions_byssvid_hourly_fishing_v20210429\\\\$20170131         --allow_large_results --use_legacy_sql=false ']\n"
     ]
    }
   ],
   "source": [
    "# utils.execute_commands_in_parallel(int_fishing_cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate positions during AIS gap events\n",
    "\n",
    "> **Note:** Interpolating positions between gap events was originally done using the `raw_gaps_vYYYYMMDD` table, which included the gaps with additional parameters applied to them - e.g. `pos_x_hours_before`. Need to produce a version of this table or interpolate the gap events as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIS Reception Quality\n",
    "\n",
    "Model AIS satellite reception quality to identify regions where AIS gap events are more/less suspicious. This is produced using the following process:\n",
    "\n",
    "**1. Calculate measured reception** - Calculates measured reception quality by AIS Class as the average number of positions received by a vessel in a day per one-degree grid cell\n",
    "\n",
    "**2. Interpolate reception** - To produce global maps of reception quality (e.g. not just in cells with AIS data) use a smoothing function to interpolate reception quality. \n",
    "\n",
    "### Create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_reception_measured = 'sat_reception_measured_one_degree_{}'.format(output_version)\n",
    "sat_reception_smoothed = 'sat_reception_smoothed_one_degree_{}'.format(output_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tables:\n",
    "    # measured reception quality\n",
    "    utils.make_bq_partitioned_table(destination_dataset, sat_reception_measured)\n",
    "    # smoothed reception quality\n",
    "    utils.make_bq_partitioned_table(destination_dataset, sat_reception_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measured reception quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of month start dates for reception quality\n",
    "reception_dates = pd.date_range(start_date, end_date, freq='1M') - pd.offsets.MonthBegin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01\n"
     ]
    }
   ],
   "source": [
    "# Generate commands\n",
    "mr_cmds = []\n",
    "for r in reception_dates:\n",
    "    print(str(r.date()))\n",
    "    cmd = utils.make_reception_measured_table(destination_table = sat_reception_measured, \n",
    "                                        destination_dataset = destination_dataset,\n",
    "                                        start_date = r, \n",
    "                                        vi_version = vi_version, \n",
    "                                        segs_table=\"{}.{}\".format(\"gfw_research\", segs_table),\n",
    "                                        output_version = output_version)\n",
    "\n",
    "    mr_cmds.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jinja2 reception/reception_measured.sql.j2           -D start_date=\"2017-01-01\"        -D end_date=\"2017-02-01\"        -D vi_version=\"v20210301\"        -D segs_table=\"gfw_research.pipe_v20201001_segs\"        -D destination_dataset=\"scratch_tyler\"        -D output_version=\"v20210429\"        |         bq query --replace         --destination_table=scratch_tyler.sat_reception_measured_one_degree_v20210429\\\\$20170101          --allow_large_results --use_legacy_sql=false']\n"
     ]
    }
   ],
   "source": [
    "utils.execute_commands_in_parallel(mr_cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothed reception quality\n",
    "\n",
    "Next, interpolate the measured reception quality using a radial basis function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/miniconda3/envs/gfw/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01\n",
      "Querying reception for 2017-01-01 00:00:00\n",
      "Interpolating reception for 2017-01-01 00:00:00\n",
      "Loaded 129600 rows for 2017-01-01 00:00:00 into sat_reception_smoothed_one_degree_v20210429.\n"
     ]
    }
   ],
   "source": [
    "for r in reception_dates:\n",
    "    print(str(r.date()))\n",
    "    utils.make_smooth_reception_table(start_date = r,\n",
    "                                      reception_measured_table = sat_reception_measured,\n",
    "                                      destination_dataset = destination_dataset,\n",
    "                                      destination_table = sat_reception_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot reception quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'plot_reception_quality'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-71e110f9179c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreception_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     utils.plot_reception_quality(reception_start_date = r,\n\u001b[0m\u001b[1;32m      4\u001b[0m                            \u001b[0mdestination_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdestination_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            reception_smoothed_table = sat_reception_smoothed)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'plot_reception_quality'"
     ]
    }
   ],
   "source": [
    "for r in reception_dates:\n",
    "    print(str(r.date()))\n",
    "    utils.plot_reception_quality(reception_start_date = r,\n",
    "                           destination_dataset = destination_dataset,\n",
    "                           reception_smoothed_table = sat_reception_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
